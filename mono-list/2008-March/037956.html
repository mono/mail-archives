<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 3.2//EN">
<HTML>
 <HEAD>
   <TITLE> [Mono-list] Threading and IO problem
   </TITLE>
   <LINK REL="Index" HREF="index.html" >
   <LINK REL="made" HREF="mailto:mono-list%40lists.ximian.com?Subject=%5BMono-list%5D%20Threading%20and%20IO%20problem&In-Reply-To=39c74bb90803061146l35ad573ck349d4eb4a837cc89%40mail.gmail.com">
   <META NAME="robots" CONTENT="index,nofollow">
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="037955.html">
   <LINK REL="Next"  HREF="037957.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[Mono-list] Threading and IO problem</H1>
    <B>Yanko Hern&#225;ndez &#193;lvarez</B> 
    <A HREF="mailto:mono-list%40lists.ximian.com?Subject=%5BMono-list%5D%20Threading%20and%20IO%20problem&In-Reply-To=39c74bb90803061146l35ad573ck349d4eb4a837cc89%40mail.gmail.com"
       TITLE="[Mono-list] Threading and IO problem">yhdezalvarez at gmail.com
       </A><BR>
    <I>Thu Mar  6 15:46:14 EST 2008</I>
    <P><UL>
        <LI>Previous message: <A HREF="037955.html">[Mono-list] Threading and IO problem
</A></li>
        <LI>Next message: <A HREF="037957.html">[Mono-list] Threading and IO problem
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#37956">[ date ]</a>
              <a href="thread.html#37956">[ thread ]</a>
              <a href="subject.html#37956">[ subject ]</a>
              <a href="author.html#37956">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>On Thu, Mar 6, 2008 at 2:46 PM, Glen Ford &lt;<A HREF="http://lists.ximian.com/mailman/listinfo/mono-list">glen.a.ford at gmail.com</A>&gt; wrote:
&gt;<i> What does top or vmstat tell you?  Are you getting IOWait?  System CPU?
</I>
No IOWait. The file is fully cached.

vmstat partial output follows:
procs -----------memory---------- ---swap-- -----io---- -system-- ----cpu----
 r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa
 8  0    192 367328     20 2556372    0    0     0     0   14 40227 63  9 28  0
 9  0    192 367280     20 2556372    0    0     0     0   25 37711 58 11 30  0
 1  0    192 367280     20 2556372    0    0     0     0   12 39163 61 12 27  0
 9  0    192 366932     20 2556376    0    0     0     0   14 35989 61 10 29  0
 2  0    192 366684     20 2556376    0    0     0     0   29 35818 62 11 28  0
 1  0    192 366436     20 2556376    0    0     0     0   11 37145 59 10 31  0
 1  0    192 366412     20 2556376    0    0     0     0   26 36032 63  9 28  0
 3  0    192 366460     20 2556376    0    0     0     0   13 36625 62 10 28  0
 8  0    192 365908     20 2556376    0    0     0    34   78 32328 61  9 30  0
 8  0    192 365948     20 2556380    0    0     0     8   33 33393 61 10 29  0
 8  0    192 365932     20 2556380    0    0     0     0   93 34001 60  9 31  0

top output (top -bHp PID) while running the program follows (two samples):
----------------
top - 15:21:07 up 9 days,  1:33,  4 users,  load average: 5.68, 4.70, 3.56
Tasks:  11 total,   8 running,   3 sleeping,   0 stopped,   0 zombie
Cpu(s): 59.5%us, 10.9%sy,  0.0%ni, 29.6%id,  0.0%wa,  0.0%hi,  0.0%si,  0.0%st
Mem:   4002052k total,  3635444k used,   366608k free,       20k buffers
Swap:  4000176k total,      192k used,  3999984k free,  2556400k cached

  PID USER      PR  NI  VIRT  RES  SHR S %CPU %MEM    TIME+  COMMAND
 3169 root      20   0  250m 9100 3244 R   61  0.2   0:29.25
MultipleReader.
 3168 root      20   0  250m 9100 3244 R   61  0.2   0:29.27
MultipleReader.
 3166 root      20   0  250m 9100 3244 R   60  0.2   0:28.86
MultipleReader.
 3167 root      20   0  250m 9100 3244 R   59  0.2   0:28.74
MultipleReader.
 3164 root      20   0  250m 9100 3244 R   59  0.2   0:28.03
MultipleReader.
 3165 root      20   0  250m 9100 3244 R   58  0.2   0:28.14
MultipleReader.
 3162 root      20   0  250m 9100 3244 R   57  0.2   0:26.89
MultipleReader.
 3163 root      20   0  250m 9100 3244 R   57  0.2   0:27.58
MultipleReader.
 3153 root      20   0  250m 9100 3244 S    3  0.2   0:01.30
MultipleReader.
 3161 root      20   0  250m 9100 3244 S    1  0.2   0:00.68
MultipleReader.
 3159 root      20   0  250m 9100 3244 S    0  0.2   0:00.00
MultipleReader.

top - 15:21:10 up 9 days,  1:33,  4 users,  load average: 5.68, 4.70, 3.56
Tasks:  11 total,   3 running,   8 sleeping,   0 stopped,   0 zombie
Cpu(s): 61.2%us,  8.6%sy,  0.0%ni, 30.3%id,  0.0%wa,  0.0%hi,  0.0%si,  0.0%st
Mem:   4002052k total,  3635244k used,   366808k free,       20k buffers
Swap:  4000176k total,      192k used,  3999984k free,  2556400k cached

  PID USER      PR  NI  VIRT  RES  SHR S %CPU %MEM    TIME+  COMMAND
 3168 root      20   0  250m 9140 3244 S   62  0.2   0:31.13
MultipleReader.
 3166 root      20   0  250m 9140 3244 S   62  0.2   0:30.71
MultipleReader.
 3169 root      20   0  250m 9140 3244 R   61  0.2   0:31.09
MultipleReader.
 3165 root      20   0  250m 9140 3244 R   59  0.2   0:29.90
MultipleReader.
 3167 root      20   0  250m 9140 3244 S   58  0.2   0:30.47
MultipleReader.
 3164 root      20   0  250m 9140 3244 R   57  0.2   0:29.75
MultipleReader.
 3162 root      20   0  250m 9140 3244 S   57  0.2   0:28.60
MultipleReader.
 3163 root      20   0  250m 9140 3244 S   57  0.2   0:29.29
MultipleReader.
 3153 root      20   0  250m 9140 3244 S    3  0.2   0:01.38
MultipleReader.
 3161 root      20   0  250m 9140 3244 S    1  0.2   0:00.72
MultipleReader.
 3159 root      20   0  250m 9+140 3244 S    0  0.2   0:00.00 MultipleReader.

Seeing the stats, it seems I overestimated the CPU use.  It's even
lower than I thought (60..70% instead of 70..80%)

&gt;<i> Do you have 8 CPUs or 8 cores?
</I>&gt;<i>
</I>
It is a Dell PowerEdge 2900 (2 xeon 5050, dual core, each core hyperthreaded)

&gt;<i> Multicore processors often have to compete for the memory bandwidth -
</I>&gt;<i> on one project we had to move from quad-core to multiple dual-core
</I>&gt;<i> because of this.
</I>
I suspect it could be the GC. but I don't know any way to corroborate
this suspicion.  This statement:

     while ((Str = SR.ReadLine()) != null) ;

should create 2.5+M strings (for the file I'm using to test this) for
every thread so the GC must be called many times.

Any way to corroborate this? Any workaround to use the all the CPU
power available?
</PRE>












<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="037955.html">[Mono-list] Threading and IO problem
</A></li>
	<LI>Next message: <A HREF="037957.html">[Mono-list] Threading and IO problem
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#37956">[ date ]</a>
              <a href="thread.html#37956">[ thread ]</a>
              <a href="subject.html#37956">[ subject ]</a>
              <a href="author.html#37956">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="http://lists.ximian.com/mailman/listinfo/mono-list">More information about the Mono-list
mailing list</a><br>
</body></html>
