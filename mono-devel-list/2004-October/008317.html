<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 3.2//EN">
<HTML>
 <HEAD>
   <TITLE> [Mono-devel-list] Two IO-Layer performance ideas
   </TITLE>
   <LINK REL="Index" HREF="index.html" >
   <LINK REL="made" HREF="mailto:mono-devel-list%40lists.ximian.com?Subject=%5BMono-devel-list%5D%20Two%20IO-Layer%20performance%20ideas&In-Reply-To=">
   <META NAME="robots" CONTENT="index,nofollow">
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="008322.html">
   <LINK REL="Next"  HREF="008318.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[Mono-devel-list] Two IO-Layer performance ideas</H1>
    <B>Ben Maurer</B> 
    <A HREF="mailto:mono-devel-list%40lists.ximian.com?Subject=%5BMono-devel-list%5D%20Two%20IO-Layer%20performance%20ideas&In-Reply-To="
       TITLE="[Mono-devel-list] Two IO-Layer performance ideas">bmaurer at ximian.com
       </A><BR>
    <I>Thu Oct 14 23:06:20 EDT 2004</I>
    <P><UL>
        <LI>Previous message: <A HREF="008322.html">[Mono-devel-list] System.Runtime.CompilerServices/IsVolatile.cs
</A></li>
        <LI>Next message: <A HREF="008318.html">[Mono-devel-list] DataSetHelper Class and Mono problems?
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#8317">[ date ]</a>
              <a href="thread.html#8317">[ thread ]</a>
              <a href="subject.html#8317">[ subject ]</a>
              <a href="author.html#8317">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>Hey guys,

I just wanted to put out two io-layer performance ideas for commenting.

Miguel was saying that io-layer is being a bit slow for beagle. Also, in
tests for Monitors, I have noticed that the performance of semaphores is
as much as 10x worse than native pthreads code.

I think much of the overhead of io-layer is coming from two areas:

1) Excess locks for getting segment data.

Today in io-layer we have code like:
        
        static inline struct _WapiHandleShared_list *_wapi_handle_get_shared_segment (guint32 segment)
        {
        	struct _WapiHandleShared_list *shared;
        	int thr_ret;
        	
        	pthread_cleanup_push ((void(*)(void *))pthread_mutex_unlock,
        			      (void *)&amp;_wapi_shared_mutex);
        	thr_ret = pthread_mutex_lock (&amp;_wapi_shared_mutex);
        	g_assert (thr_ret == 0);
        	
        	shared=_wapi_shared_data[segment];
        
        	thr_ret = pthread_mutex_unlock (&amp;_wapi_shared_mutex);
        	g_assert (thr_ret == 0);
        	pthread_cleanup_pop (0);
        
        	return(shared);
        }

This code must be called each time we use a handle (basically). The
reason we must lock is because _wapi_shared_data needs to be dynamically
expanded.

I think we can do a clever trick here. We can say

        #define NUM_FASTPATH_SEGMENTS /* some number */
        
        struct _WapiHandleShared_list * _wapi_fast_shared_data [NUM_FASTPATH_SEGMENTS];
        
If `segment &lt; NUM_FASTPAH_SEGMENTS' we can just use the index of
_wapi_fast_shared_data. Because that area is statically allocated, we
would not have to lock it. We can assume that in most cases, there will
not be more than a few segments (we need real life data for the number).

This should cut a little overhead from all the io type functions.
Really, the benefit of this path is for smp boxes with multiple threads
running. We reduce the risk that someone will need to block on a thread.

2) Make the _wapi_handle_[un]ref functions in-process

Today to ref or unref a handle, we must do IPC. We should be using
Interlocked type functions to do the refcounting. If the refcount goes
to 0, we can do the ipc.

The only problem I saw here is that the daemon does:
        static void ref_handle (ChannelData *channel_data, guint32 handle)
        {
        	guint32 segment, idx;
        	
        	if(handle==0) {
        		return;
        	}
        	
        	_wapi_handle_segment (GUINT_TO_POINTER (handle), &amp;segment, &amp;idx);
        	
        	_wapi_shared_data[segment]-&gt;handles[idx].ref++;
        	channel_data-&gt;open_handles[handle]++;

The _wapi_shared_data is easy to do with interlocked, because we have
the data in shared memory. However, the channel data is not. I am not
sure what we want to do with this.

I know that we are going to rewrite things to remove the daemon.
However, this function accounts for much of the overhead of the daemon
at runtime. Also, rewriting this function should be a
step-in-the-right-direction.

I think I know how to do 1. I will try cooking a patch over the weekend.
However, 2 I need some more input on.

-- 
Ben Maurer &lt;<A HREF="http://lists.ximian.com/mailman/listinfo/mono-devel-list">bmaurer at ximian.com</A>&gt;


</PRE>

<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="008322.html">[Mono-devel-list] System.Runtime.CompilerServices/IsVolatile.cs
</A></li>
	<LI>Next message: <A HREF="008318.html">[Mono-devel-list] DataSetHelper Class and Mono problems?
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#8317">[ date ]</a>
              <a href="thread.html#8317">[ thread ]</a>
              <a href="subject.html#8317">[ subject ]</a>
              <a href="author.html#8317">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="http://lists.ximian.com/mailman/listinfo/mono-devel-list">More information about the Mono-devel-list
mailing list</a><br>
</body></html>
